agent:
  description: >
    You are building a RAG (Retrieval-Augmented Generation) backend system using Supabase, OpenAI, and Edge Functions.
    The project will ingest band/artist-related JSON data from a set of public API endpoints listed on https://elgoose.net/api/docs/.
    Your job is to fetch and process this data using Supabase Edge Functions, generate OpenAI embeddings, and store everything in a vectorized documents table.
    You are working in a local environment using Cursor, but the Supabase database is hosted live.

  project_overview: >
    The goal of the project is to build a backend pipeline that:
      - Automatically ingests JSON data from a set of predefined API endpoints
      - Embeds that data using OpenAI embeddings (text-embedding-3-small)
      - Stores the chunks and metadata in a Supabase `documents` table
      - Uses Supabase Edge Functions to fetch, process, embed, and insert the data
    This project will later power a RAG chatbot frontend, but for now focuses only on the backend pipeline.

 Project Title: Band Chatbot RAG Pipeline (Supabase + OpenAI)
________________________________________
ğŸ”„ Project Overview
This project builds a RAG (Retrieval-Augmented Generation) pipeline using: - Supabase (PostgreSQL + pgvector + Edge Functions) - OpenAI (for embeddings and chat completions) - ElGoose API (source of music-related JSON data)
The goal is to ingest structured JSON data from multiple ElGoose API endpoints, embed the content, and store it in a Supabase vector store for semantic retrieval.
________________________________________
âš–ï¸ Technologies Used
â€¢	Supabase (PostgreSQL, pgvector, Edge Functions, Supabase Auth)
â€¢	Deno (Edge Functions)
â€¢	OpenAI API (text-embedding-3-small, gpt-4)
â€¢	JSON chunking + semantic search
â€¢	Cursor IDE for local dev
â€¢	.env for secrets management
________________________________________
ğŸ“‹ Development Plan
We will proceed in the following order. All steps should be modular and testable individually.
PHASE 1: Backend Setup
Goal: Connect to Supabase and prepare database schema - [ ] Create Supabase project & connect to Cursor - Open Supabase â†’ New project (Postgres 15+) - Copy Project URL, Anon key, Service Role key (Settings â†’ API) - Create .env locally with SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, OPENAI_API_KEY - Install Supabase CLI (npm i -g supabase) and run supabase init - [ ] Enable pgvector - In Supabase SQL Editor, run: create extension if not exists vector; - (Optional) verify with: select extname from pg_extension where extname='vector'; - [ ] Create documents table (vector store) - Columns: id uuid pk default gen_random_uuid(), content text, embedding vector(1536), metadata jsonb, content_hash text, source_method text, source_key text, created_at timestamptz default now() - Indexes: create index on documents using ivfflat (embedding vector_cosine_ops) with (lists = 100); and btree indexes on source_method, source_key - RLS (optional now, required later): enable table RLS; add policies when auth is added - [ ] Create api_sources table (drives ingestion) - Columns: id uuid pk, name text, url text unique, description text, is_active boolean default true, created_at timestamptz default now() - Seed with the initial ElGoose endpoints you want to ingest - [ ] Add RPC for similarity search - match_documents(query_embedding vector, match_threshold float, match_count int) returning id, content, metadata, similarity - Use cosine similarity: 1 - (embedding <=> query_embedding) - [ ] Create schema.sql & apply - Save all the above SQL in supabase/schema.sql - Apply locally via CLI: supabase db push (or run in the dashboard SQL Editor)
PHASE 2: API Source Loading
API Source Loading Goal:** Set up ElGoose API ingestion logic - [ ] Add each endpoint (URL + name) to api_sources - [ ] Create a test script or function to fetch from a single endpoint - [ ] Validate and normalize JSON schema
PHASE 3: Edge Function to Embed Data
Goal: Create Supabase Edge Function that fetches, chunks, and embeds data - [ ] Create update_json_data edge function - [ ] Load environment variables using Deno.env.get() - [ ] Loop through active endpoints - [ ] Chunk each JSON object (by field length or tokens) - [ ] Call OpenAI embedding endpoint - [ ] Insert embedded chunks into documents
PHASE 4: Retrieval & Query Function
Goal: Build RAG-compatible retrieval endpoint - [ ] Add function to embed user query - [ ] Search documents table using pgvector similarity - [ ] Return top-k matching content
PHASE 5: Frontend & Auth (Later)
Goal: Expose a chatbot frontend - [ ] Use Supabase Auth for user login - [ ] Build simple chatbot UI (Next.js or other) - [ ] Call backend retrieval function - [ ] Display results and sources
________________________________________
ğŸ“‚ Folder Structure (Planned)
rag-chatbot/
â”œâ”€â”€ supabase/
â”‚   â”œâ”€â”€ schema.sql
â”‚   â””â”€â”€ functions/
â”‚        â””â”€â”€ update_json_data/index.ts
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ embed_json.py
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ query_embed.py
â”œâ”€â”€ test_data/
â”‚   â””â”€â”€ example_api_response.json
â”œâ”€â”€ .env.template
â”œâ”€â”€ cursor.yml
â””â”€â”€ README.md
________________________________________
ğŸ‘ï¸ Environment Variables (.env)
SUPABASE_URL=https://<your-project-id>.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-secret-role-key
OPENAI_API_KEY=sk-xxx
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536
PORT=3000
NODE_ENV=development
________________________________________
ğŸ“œ ElGoose API Endpoints (ingestion context)
All endpoints are under https://elgoose.net/api/v2 and support .json. Each returns an envelope { error, error_message, data }. Store only data rows.
â€¢	setlists â€” song-by-song entries for shows; filter examples: setlists/showyear/2024.json, setlists/songname/<Name>.json.
Chunk: one row per setlist line; include show date, set, position, song name, venue/city/state; add footnotes/jam notes if present.
Metadata: show_id, song_slug, venue_slug, city/state/country.
â€¢	latest â€” recent setlist lines in recency order; supports order_by, direction, limit.
Use for deltas; same chunking as setlists.
â€¢	shows â€” one row per show (date, artist, venue, location).
Chunk: one per show; concise narrative string.
Metadata: show_id, artist, venue_slug, permalink.
â€¢	songs â€” song catalog (name, slug, attributes).
Chunk: one per song; helpful for enrichment & disambiguation.
Metadata: song_id, song_slug.
â€¢	venues â€” venue directory; supports venues.json, venues/state/<STATE>.json, venues/city/<City>.json.
Chunk: one per venue (name + location).
Metadata: venue_slug.
â€¢	jamcharts â€” curated performance notes with jamchartnote, tracktime, set/position, show context.
Chunk: one per jam entry (high-value text).
Metadata: song_slug, showdate, venue_slug, permalink, tracktime.
â€¢	metadata â€” extra metadata tied to songs/setlists; e.g., by song_slug.
Chunk: one per row of descriptive text.
Metadata: song_slug, type.
â€¢	links â€” external links for shows; e.g., links/show_id/<id>.json.
Chunk: one per link (title/desc).
Metadata: show_id, url, link_type.
â€¢	uploads â€” show posters/featured media.
Chunk: one per upload (caption/desc).
Metadata: show_id, asset URLs.
â€¢	appearances â€” which person appeared at which show.
Chunk: one per appearance.
Metadata: person_id, show_id.
â€¢	list â€” enumerations for filters (years, cities, states, countries, venues).
Not embedded; use to plan fetch loops.
General ingestion rules - Use api_sources to store the exact URLs you want crawled (plus is_active). - Normalize each data object â†’ build a content string (for embedding) + metadata json. - Compute content_hash to skip duplicates. Store source_method (e.g., setlists) and a stable source_key (show_id or composite). - Prefer incremental runs: poll latest frequently; backfill setlists by year with limits.
________________________________________
ğŸ” References & Resources
â€¢	ElGoose API Docs: https://elgoose.net/api/docs/
â€¢	Supabase Docs: https://supabase.com/docs
â€¢	OpenAI Docs: https://platform.openai.com/docs
â€¢	pgvector: https://github.com/pgvector/pgvector
________________________________________
This doc is designed to be both a guide for you and context for your Cursor AI agent. Update it frequently as the project evolves.
